{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmdfWoy1vjN9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import psutil\n",
        "import numpy as np  # import auxiliary library, typical idiom\n",
        "import pandas as pd  # import the Pandas library, typical idiom\n",
        "from pandas import read_csv\n",
        "import statsmodels.api as sm\n",
        "import time\n",
        "import pm4py\n",
        "from datetime import datetime\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "\n",
        "from numba import jit\n",
        "\n",
        "from sklearn.linear_model import LinearRegression  # for linear regression\n",
        "from sklearn import linear_model\n",
        "from sklearn.cluster import KMeans  # for clustering\n",
        "from sklearn.tree import DecisionTreeClassifier  # for decision tree mining\n",
        "from sklearn.metrics import mean_absolute_error, confusion_matrix, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.formula.api as smf \n",
        "import statsmodels.api as sm\n",
        "from statsmodels.graphics.gofplots import ProbPlot\n",
        "from matplotlib import pyplot\n",
        "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_selection import SelectFromModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwzwRVG_vhqh"
      },
      "outputs": [],
      "source": [
        "file_export = 'export2018.csv'\n",
        "data = pd.read_csv(file_export)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing"
      ],
      "metadata": {
        "id": "mYVm3bvcETx5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_zL5ww6aO1w"
      },
      "source": [
        "Pre-processing\n",
        "* Visualization\n",
        "1. Unix time\n",
        "2. Encoding of categorical features\n",
        "3. Temporal ordering\n",
        "4. Aditional features:\n",
        "- Previous event\n",
        "- Next event\n",
        "- Day of the week\n",
        "- Time of day\n",
        "- Event duration\n",
        "5. Separate 80-20 \n",
        "- Visualization\n",
        "6. Get rid of overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce6x4-68AYCp"
      },
      "outputs": [],
      "source": [
        "data = data.sort_values(by=['case','startTime'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkLk7PX7CvL4"
      },
      "outputs": [],
      "source": [
        "#Duration\n",
        "@jit(parallel = True)\n",
        "def calculator_nb(case, startTime):\n",
        "    res = np.empty(len(case), dtype=object)\n",
        "    idx = 0\n",
        "    for _ in case:\n",
        "        if (idx+1 >= len(case)):\n",
        "            break\n",
        "\n",
        "        if (case[idx + 1] == case[idx]):\n",
        "            res[idx] = startTime[idx + 1]\n",
        "        else:\n",
        "            res[idx] = startTime[idx]\n",
        "\n",
        "        idx+=1\n",
        "    return res\n",
        "\n",
        "data['completeTime'] = calculator_nb(data['case'].values, data['startTime'].values)\n",
        "data.at[317373, 'completeTime'] = data.at[317373, 'startTime']\n",
        "\n",
        "data['startTime'] =  pd.to_datetime(data['startTime'])\n",
        "data['completeTime'] =  pd.to_datetime(data['completeTime'])\n",
        "data['duration'] = data['completeTime'] - data['startTime']\n",
        "#to turn duration into seconds:\n",
        "duration = data['duration']\n",
        "duration = duration / np.timedelta64(1, 's')\n",
        "data['duration'] = duration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyyc-XNsAYCr"
      },
      "outputs": [],
      "source": [
        "#Next event\n",
        "@jit(parallel = True)\n",
        "def calculator_nb(case, event):\n",
        "    res = np.empty(len(case), dtype=object)\n",
        "    idx = 0\n",
        "    for _ in case:\n",
        "        if (idx+1 >= len(case)):\n",
        "            break\n",
        "       \n",
        "        if (case[idx + 1] == case[idx]):\n",
        "            res[idx] = event[idx + 1]\n",
        "\n",
        "        idx+=1\n",
        "    return res\n",
        "\n",
        "data['next_event'] = calculator_nb(data['case'].values, data['event'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxseoX0JAYCr"
      },
      "outputs": [],
      "source": [
        "#Previous event\n",
        "@jit(parallel = True)\n",
        "def calculator_nb(case, event):\n",
        "    res = np.empty(len(case), dtype=object)\n",
        "    idx = 0\n",
        "    for _ in case:\n",
        "        if (idx+1 >= len(case)):\n",
        "            break\n",
        "       \n",
        "        if (case[idx + 1] == case[idx]):\n",
        "            res[idx + 1] = event[idx]\n",
        "\n",
        "        idx+=1\n",
        "    return res\n",
        "\n",
        "data['prev_event'] = calculator_nb(data['case'].values, data['event'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lseT5ybaAYCs"
      },
      "outputs": [],
      "source": [
        "#Removing null values\n",
        "data['next_event'] = data['next_event'].fillna(value='None')\n",
        "data['prev_event'] = data['prev_event'].fillna(value='None')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EK0yT10d4erc"
      },
      "outputs": [],
      "source": [
        "#unix time\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "\n",
        "data['startTime'] = pd.to_datetime(data['startTime'], dayfirst=True)\n",
        "unixTransform = lambda x: time.mktime(x.timetuple())\n",
        "data[\"UNIX_starttime\"] = data[\"startTime\"].apply(unixTransform).astype(int)\n",
        "\n",
        "data['completeTime'] = pd.to_datetime(data['completeTime'], dayfirst=True)\n",
        "unixTransform = lambda x: time.mktime(x.timetuple())\n",
        "data[\"UNIX_completeTime\"] = data[\"completeTime\"].apply(unixTransform).astype(int)\n",
        "\n",
        "#data['REG_DATE'] = pd.to_datetime(data['REG_DATE'], dayfirst=True)\n",
        "#unixTransform = lambda x: time.mktime(x.timetuple())\n",
        "#data[\"UNIX_REG_DATE\"] = data[\"REG_DATE\"].apply(unixTransform).astype(int)\n",
        "\n",
        "#print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YGUvDZrAYCt"
      },
      "outputs": [],
      "source": [
        "#Day of the week\n",
        "data['weekday'] = data['startTime'].dt.dayofweek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78FN-5HNNi4f"
      },
      "outputs": [],
      "source": [
        "#encoding of categorical data\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "label_encoder = LabelEncoder()\n",
        "data['enc_event'] = ordinal_encoder.fit_transform(data[['event']]).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l7QS1P6ndTr"
      },
      "outputs": [],
      "source": [
        "#ensure we have acces to orignal indexing to keep track of the order of events in a process\n",
        "data['original index'] = data.index\n",
        "\n",
        "#sorting on time\n",
        "data.sort_values(by = \"UNIX_starttime\", ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5nloqg7KspI"
      },
      "outputs": [],
      "source": [
        "#separation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.2, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDODvtbAJZad"
      },
      "outputs": [],
      "source": [
        "#removing overlap - if case is in both datasets, remove\n",
        "\n",
        "train_cases = train['case'].unique().tolist()\n",
        "test_cases = test['case'].unique().tolist()\n",
        "\n",
        "intersect_list = list(set(train_cases).intersection(test_cases))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9a3TLae6Vgc"
      },
      "outputs": [],
      "source": [
        "#only removes first value in intersect list (needs modification for multiple overlaping values)\n",
        "\n",
        "#train = train[train['case'] != intersect_list[0]]\n",
        "#test = test[test['case'] != intersect_list[0]]\n",
        "\n",
        "#works for more values\n",
        "org_train = train.copy()\n",
        "org_test = test.copy()\n",
        "df_ordinal_encoder = LabelEncoder()\n",
        "train=train.apply(df_ordinal_encoder.fit_transform)\n",
        "test=test.apply(df_ordinal_encoder.fit_transform)\n",
        "\n",
        "train = train[train['case'].isin(intersect_list) == False]\n",
        "X_train_time = train.drop(columns='duration')\n",
        "Y_train_time = train[\"duration\"]\n",
        "X_train_event = train.drop(columns=[\"next_event\"])\n",
        "Y_train_event = train[\"event\"]\n",
        "\n",
        "test = test[test['case'].isin(intersect_list) == False]\n",
        "X_test_time = test.drop(columns='duration')\n",
        "Y_test_time = test[\"duration\"]\n",
        "X_test_event = test.drop(columns=['next_event'])\n",
        "Y_test_event = test[\"event\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#separation visualisation\n",
        "\n",
        "g = sns.scatterplot(x=\"UNIX_starttime\", y=\"case\", hue=\"enc_event\", data=data, palette='colorblind', legend=False)\n",
        "\n",
        "#add lines for separation - horizontal and vertical"
      ],
      "metadata": {
        "id": "fCxZL9hWDs33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJeqDuL6AYCv",
        "outputId": "60ac9040-5ca1-4e0c-c135-a0b9d38c39d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['case', 'event', 'startTime', 'completeTime', 'penalty_JLP1',\n",
              "       'penalty_JLP3', 'number_parcels', 'penalty_JLP2', 'penalty_JLP5',\n",
              "       'year', 'penalty_JLP7', 'penalty_JLP6', 'redistribution',\n",
              "       'amount_applied1', 'amount_applied0', 'amount_applied3',\n",
              "       'amount_applied2', 'identity:id', 'penalty_V5', 'payment_actual0',\n",
              "       'payment_actual2', 'payment_actual1', 'penalty_B5F', 'payment_actual3',\n",
              "       'penalty_B16', 'penalty_GP1', 'basic payment', 'penalty_AGP', 'area',\n",
              "       'selected_manually', 'penalty_B3', 'penalty_B2', 'selected_risk',\n",
              "       'penalty_B5', 'penalty_AVBP', 'penalty_B4', 'penalty_B6', 'penalty_ABP',\n",
              "       'penalty_AVGP', 'penalty_C4', 'greening', 'rejected',\n",
              "       'cross_compliance', 'penalty_C9', 'penalty_AVJLP', 'penalty_CC',\n",
              "       'penalty_AVUVP', 'penalty_BGK', 'penalty_C16', 'penalty_BGP',\n",
              "       'department', 'small farmer', 'risk_factor', 'applicant',\n",
              "       'penalty_AUVP', 'penalty_amount2', 'penalty_BGKV', 'penalty_amount3',\n",
              "       'application', 'penalty_amount0', 'program-id', 'penalty_amount1',\n",
              "       'penalty_AJLP', 'selected_random', 'young farmer', 'note', 'eventid',\n",
              "       'activity', 'docid', 'subprocess', 'event_identity:id', 'doctype',\n",
              "       'docid_uuid', 'org:resource', 'success', 'duration', 'next_event',\n",
              "       'prev_event', 'UNIX_starttime', 'UNIX_completeTime', 'weekday',\n",
              "       'enc_event', 'original index'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9BgcNRxAYCw"
      },
      "source": [
        "# Feature prediction for time and event based on KBest(z-scores)\n",
        "note: don't run takes a significant time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPCfrgnjAYCw"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "select = SelectKBest(k=10) # takes best 10 arguments \n",
        "z = select.fit_transform(X_train_time, Y_train_time)\n",
        "filter = select.get_support()\n",
        "print(np.extract(filter, train.columns))\n",
        "# ['event' 'selected_random' 'note' 'eventid' 'activity' 'subprocess' 'org:resource' 'duration' 'next_event' 'weekday'] for time\n",
        "\n",
        "\n",
        "\n",
        "select = SelectKBest(k=10) # takes best 10 arguments for event\n",
        "z = select.fit_transform(X_train_event, Y_train_event)\n",
        "filter = select.get_support()\n",
        "print(np.extract(filter, train.columns)) \n",
        "# ['penalty_B2' 'penalty_AJLP' 'young farmer' 'note' 'eventid' 'docid' 'docid_uuid' 'success' 'duration' 'UNIX_completeTime'] for event"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Baseline Models"
      ],
      "metadata": {
        "id": "HnszwsuyCw71"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIsCchagAYCx"
      },
      "outputs": [],
      "source": [
        "# Naive event (needs restructuring)\n",
        "data_baseline= test.copy()\n",
        "\n",
        "@jit(parallel = True)\n",
        "def calculator_pos(case):\n",
        "    res = np.empty(len(case), dtype=object)\n",
        "    idx = 0\n",
        "    count=1\n",
        "    for _ in case:\n",
        "        if (idx+1 >= len(case)):\n",
        "            break\n",
        "       \n",
        "        if (case[idx] == case[idx-1]):\n",
        "            count+=1\n",
        "            res[idx] = count\n",
        "            \n",
        "        else:\n",
        "            count=1\n",
        "            res[idx]=count\n",
        "\n",
        "        idx+=1\n",
        "    res[-1]=count+1\n",
        "    return res\n",
        "\n",
        "data_baseline[\"pos\"] = calculator_pos(data_baseline['case'].values)\n",
        "\n",
        "event_to_num = {}\n",
        "list_of_events = train[\"event\"].unique()\n",
        "i=0\n",
        "for event in list_of_events:\n",
        "    event_to_num[str(event)] = i\n",
        "    i += 1\n",
        "event_to_num['None'] = i\n",
        "\n",
        "pop=data_baseline.sort_values(by='pos')\n",
        "pop['eventnum']=pop['enc_event']\n",
        "pop2=pop.set_index('pos')\n",
        "pop3=pop[['pos','eventnum']]\n",
        "pop4=pop3.groupby(['pos', 'eventnum']).apply(pd.DataFrame.mode).reset_index(drop=True)\n",
        "pop5=pop4.drop_duplicates(subset='pos')\n",
        "ptenum= dict(zip(pop5.pos, pop5.eventnum))\n",
        "num_to_event = {value:key for key, value in event_to_num.items()}\n",
        "data_baseline['predicted_event_num'] = (data_baseline['pos']+1).map(ptenum)\n",
        "data_baseline['predicted_event'] = (data_baseline['predicted_event_num']).map(num_to_event)\n",
        "data_baseline_final=data_baseline.drop(['predicted_event_num'],axis=1)\n",
        "\n",
        "next_task=[]\n",
        "predicted_event=[]\n",
        "for event in data_baseline_final['next_event']:\n",
        "    next_task.append(str(event))\n",
        "    \n",
        "\n",
        "for case in data_baseline_final['predicted_event']:\n",
        "    predicted_event.append(str(case))\n",
        "\n",
        "accuracy_score(next_task,predicted_event)\n",
        "\n",
        "\n",
        "test[\"naive_event\"] = predicted_event"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXmI8N5fAYCx"
      },
      "outputs": [],
      "source": [
        "# Naive time\n",
        "#Sums up count for each event and the time each event takes\n",
        "events_count = train.groupby(\"event\")['duration'].agg('count')\n",
        "event_duration_sum = train.groupby(\"event\")['duration'].agg('sum')\n",
        "\n",
        "#Computes average duration per event (basically our trained data that can be mapped onto test data)\n",
        "duration_per_event = event_duration_sum / events_count \n",
        "\n",
        "test[\"naive_time\"] = test['event'].map(duration_per_event)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "vzkbO8ioDJXG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXp-EEClAYCy"
      },
      "outputs": [],
      "source": [
        "# Random forest event\n",
        "DT = DecisionTreeClassifier()\n",
        "RF = RandomForestClassifier()\n",
        "\n",
        "DT_fit = DT.fit(X_train_event.iloc[:1000], Y_train_event.iloc[:1000])\n",
        "RF_fit = RF.fit(X_train_event['event'].iloc[:1000], Y_train_event.iloc[:1000])\n",
        "\n",
        "DT_pred = DT_fit.predict(X_test_event.iloc[:1000])\n",
        "RF_pred = RF_fit.predict(X_test_event['event'].iloc[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "eJ95WWv1FFq2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEjA1xPPAYCz"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-7Z14JuAYCz",
        "outputId": "e9f6193f-b3bb-4f04-d25d-44df26e955ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['case', 'event', 'startTime', 'completeTime', 'penalty_JLP1',\n",
              "       'penalty_JLP3', 'number_parcels', 'penalty_JLP2', 'penalty_JLP5',\n",
              "       'year', 'penalty_JLP7', 'penalty_JLP6', 'redistribution',\n",
              "       'amount_applied1', 'amount_applied0', 'amount_applied3',\n",
              "       'amount_applied2', 'identity:id', 'penalty_V5', 'payment_actual0',\n",
              "       'payment_actual2', 'payment_actual1', 'penalty_B5F', 'payment_actual3',\n",
              "       'penalty_B16', 'penalty_GP1', 'basic payment', 'penalty_AGP', 'area',\n",
              "       'selected_manually', 'penalty_B3', 'penalty_B2', 'selected_risk',\n",
              "       'penalty_B5', 'penalty_AVBP', 'penalty_B4', 'penalty_B6', 'penalty_ABP',\n",
              "       'penalty_AVGP', 'penalty_C4', 'greening', 'rejected',\n",
              "       'cross_compliance', 'penalty_C9', 'penalty_AVJLP', 'penalty_CC',\n",
              "       'penalty_AVUVP', 'penalty_BGK', 'penalty_C16', 'penalty_BGP',\n",
              "       'department', 'small farmer', 'risk_factor', 'applicant',\n",
              "       'penalty_AUVP', 'penalty_amount2', 'penalty_BGKV', 'penalty_amount3',\n",
              "       'application', 'penalty_amount0', 'program-id', 'penalty_amount1',\n",
              "       'penalty_AJLP', 'selected_random', 'young farmer', 'note', 'eventid',\n",
              "       'activity', 'docid', 'subprocess', 'event_identity:id', 'doctype',\n",
              "       'docid_uuid', 'org:resource', 'success', 'duration', 'next_event',\n",
              "       'prev_event', 'UNIX_starttime', 'UNIX_completeTime', 'weekday',\n",
              "       'enc_event', 'original index'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "listVal = train\n",
        "listVal.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB-xrCM9AYCz"
      },
      "outputs": [],
      "source": [
        "columnNames = ['case', 'event', 'UNIX_starttime', 'UNIX_completeTime', 'weekday'] # chose better features\n",
        "\n",
        "listValSelected = listVal[columnNames]\n",
        "listValSelected_prediction = train[columnNames]\n",
        "listValSelected_prediction = listValSelected_prediction.values\n",
        "listValDuration_prediction = org_train['duration']\n",
        "listValDuration_prediction = listValDuration_prediction.values\n",
        "listValDuration = listVal['duration'].values\n",
        "\n",
        "listValSelected = listValSelected.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxFBpvJrAYC0",
        "outputId": "afef450d-4463-4567-a3a0-8f40a49df81a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62857/62857 [==============================] - 382s 6ms/step - loss: 174656913408.0000\n"
          ]
        }
      ],
      "source": [
        "# choose a number of time steps\n",
        "n_steps = len(listValSelected[0])\n",
        "# split into samples\n",
        "\n",
        "\n",
        "n_features = 1\n",
        "X = listValSelected.reshape((listValSelected.shape[0], listValSelected.shape[1], n_features))\n",
        "y = listValDuration\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(n_steps, n_features), return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "# Dropout for regularization\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "model.fit(X, y, epochs=1, verbose=1, workers=-1)\n",
        "\n",
        "# demonstrate prediction\n",
        "x_input = listValSelected_prediction\n",
        "\n",
        "x_input = x_input.reshape((x_input.shape[0], n_steps, n_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHoM863CAYC0",
        "outputId": "01d5989f-fd7a-48b8-fd4c-b8dfa3e5863b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62857/62857 - 88s - 88s/epoch - 1ms/step\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'values'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1744/2639317476.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlenVal_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistValDuration_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistValDuration_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlenVal_len\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 80549.086 seconds which is 5 minutes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# test_df[\"duration_prediction\"] = yhat.flatten()[:len(test_df)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
          ]
        }
      ],
      "source": [
        "yhat = model.predict(x_input.astype('float32'), verbose=2)\n",
        "lenVal_len = len(listValDuration_prediction) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB6Jv4HtAYC0",
        "outputId": "61e883b2-a1c6-44fd-d818-71496be2106f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "504639.3534027153\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(mean_absolute_error(listValDuration_prediction, yhat.flatten()[:lenVal_len])) # 504639.3534027153 seconds\n",
        "# test_df[\"duration_prediction\"] = yhat.flatten()[:len(test_df)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "G8VnfD11COk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(df_name, col_name):\n",
        "    col_as_array = df_name[col_name].to_numpy()\n",
        "    col_as_array = np.where(col_as_array == 0, 0.01, col_as_array)\n",
        "    col_as_array_norm = np.log10(col_as_array)\n",
        "    mean = col_as_array_norm.mean()\n",
        "    stdev = col_as_array_norm.std()\n",
        "    epsilon = 0.01\n",
        "    return (col_as_array_norm - mean) / (stdev + epsilon)"
      ],
      "metadata": {
        "id": "4mAQfBJXA5e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepfeatures(df_name):\n",
        "    event = df_name['enc_event'].to_numpy()\n",
        "    event = event.reshape(-1,1)\n",
        "    \n",
        "    duration = normalize(df_name,'duration')\n",
        "    startTime = normalize(df_name,'UNIX_starttime')\n",
        "    weekday = df_name['weekday'].to_numpy()\n",
        "    \n",
        "    prev_event = df_name['prev_event'].to_numpy()\n",
        "    prev_event = prev_event.reshape(-1,1)\n",
        "    prev_event = ordinal_encoder.fit_transform(prev_event)\n",
        "    \n",
        "    features = []\n",
        "    for i in range(len(event)):\n",
        "        current = event[i]\n",
        "        current = np.append(current,duration[i])\n",
        "        current = np.append(current,startTime[i])\n",
        "        current = np.append(current,prev_event[i])\n",
        "        current = np.append(current,weekday[i])\n",
        "        features.append(current)\n",
        "        \n",
        "    return np.array(features)"
      ],
      "metadata": {
        "id": "gjaBr85RBK9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preplabels(df_name):\n",
        "    labels = df_name['next_event'].to_numpy()\n",
        "    labels = label_encoder.fit_transform(labels)\n",
        "    labels = labels.reshape(-1, 1)\n",
        "    \n",
        "    return np.array(labels)"
      ],
      "metadata": {
        "id": "OtJzg5ZPBNhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = prepfeatures(train)\n",
        "labels = preplabels(train)"
      ],
      "metadata": {
        "id": "On8mJnByBPbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(5,)),\n",
        "    keras.layers.Dense(10, activation='softplus'),\n",
        "    keras.layers.Dense(15, activation='softplus'),\n",
        "    keras.layers.Dense(20, activation='softplus'),\n",
        "    keras.layers.Dropout(1/20),\n",
        "    keras.layers.Dense(25, activation='softplus'),\n",
        "    keras.layers.Dense(30, activation='softplus'),\n",
        "    keras.layers.Dropout(1/30),\n",
        "    keras.layers.Dense(35, activation='softplus'),\n",
        "    keras.layers.Dense(42, activation='softplus')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "szrad1aBBRNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(features,labels,epochs=5,verbose=1)"
      ],
      "metadata": {
        "id": "NCwPArxCBTj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_test = prepfeatures(test)\n",
        "labels_test = preplabels(test)"
      ],
      "metadata": {
        "id": "xgYm7vyHBWYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['neuralnet_event'] = model.predict(features_test)"
      ],
      "metadata": {
        "id": "G02fgUc3BjNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "bMwKREQNFNMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepfeatures_OLS(df_name):\n",
        "    startTime = pd.to_datetime(df_name['startTime'])\n",
        "    \n",
        "    for i in range(len(startTime)):\n",
        "        startTime[i] = startTime[i].timestamp()\n",
        "        \n",
        "    startTime = np.array(startTime).reshape(-1,1)\n",
        "    \n",
        "    event = df_name['event'].to_numpy()\n",
        "    event = event.reshape(-1,1)\n",
        "    event = ordinal_encoder.fit_transform(event)\n",
        "    \n",
        "    #payment_actual0 = normalize(df_name,'payment_actual0')\n",
        "    #penalty_amount0 = normalize(df_name,'penalty_amount0')\n",
        "    #number_parcels = normalize(df_name,'number_parcels')\n",
        "    #area = normalize(df_name,'area')\n",
        "    payment_actual0 = df_name['payment_actual0'].to_numpy()\n",
        "    penalty_amount0 = df_name['penalty_amount0'].to_numpy()\n",
        "    number_parcels = df_name['number_parcels'].to_numpy()\n",
        "    area = df_name['area'].to_numpy()\n",
        "    \n",
        "    X = []\n",
        "    for i in range(len(event)):\n",
        "        current = startTime[i]\n",
        "        current = np.append(current, event[i])\n",
        "        current = np.append(current, payment_actual0[i])\n",
        "        current = np.append(current, penalty_amount0[i])\n",
        "        current = np.append(current, number_parcels[i])\n",
        "        current = np.append(current, area[i])\n",
        "        X.append(current)\n",
        "        \n",
        "    return np.array(X, dtype=float)"
      ],
      "metadata": {
        "id": "ujVonZkbFUFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preplabels_OLS(df_name):\n",
        "    duration = df_name['duration'].to_numpy()\n",
        "    return np.array(duration, dtype=float)"
      ],
      "metadata": {
        "id": "Y_VXHw5DFaAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = prepfeatures_OLS(train)\n",
        "y = preplabels_OLS(train)\n",
        "\n",
        "huber = HuberRegressor().fit(X, y)\n",
        "\n",
        "X_test = prepfeatures_OLS(test)\n",
        "\n",
        "test['regression_duration'] = huber.predict(X_test)\n",
        "test['error'] = np.absolute(test['duration'] - test['regression_duration'])\n",
        "test['error'].mean()"
      ],
      "metadata": {
        "id": "M0Qh176sFcMT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Sprint3",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}