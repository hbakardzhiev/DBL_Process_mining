{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmdfWoy1vjN9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "import numpy as np  # import auxiliary library, typical idiom\n",
    "import pandas as pd  # import the Pandas library, typical idiom\n",
    "from pandas import read_csv\n",
    "import statsmodels.api as sm\n",
    "import time\n",
    "import pm4py\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "from sklearn.linear_model import LinearRegression  # for linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.cluster import KMeans  # for clustering\n",
    "from sklearn.tree import DecisionTreeClassifier  # for decision tree mining\n",
    "from sklearn.metrics import mean_absolute_error, confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf \n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.linear_model import HuberRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwzwRVG_vhqh"
   },
   "outputs": [],
   "source": [
    "file_export = 'export2018.csv'\n",
    "data = pd.read_csv(file_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYVm3bvcETx5"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_zL5ww6aO1w"
   },
   "source": [
    "Pre-processing\n",
    "* Visualization\n",
    "1. Unix time\n",
    "2. Encoding of categorical features\n",
    "3. Temporal ordering\n",
    "4. Aditional features:\n",
    "- Previous event\n",
    "- Next event\n",
    "- Day of the week\n",
    "- Time of day\n",
    "- Event duration\n",
    "5. Separate 80-20 \n",
    "- Visualization\n",
    "6. Get rid of overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ce6x4-68AYCp"
   },
   "outputs": [],
   "source": [
    "data = data.sort_values(by=['case','startTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkLk7PX7CvL4"
   },
   "outputs": [],
   "source": [
    "#Duration\n",
    "@jit(parallel = True)\n",
    "def calculator_nb(case, startTime):\n",
    "    res = np.empty(len(case), dtype=object)\n",
    "    idx = 0\n",
    "    for _ in case:\n",
    "        if (idx+1 >= len(case)):\n",
    "            break\n",
    "\n",
    "        if (case[idx + 1] == case[idx]):\n",
    "            res[idx] = startTime[idx + 1]\n",
    "        else:\n",
    "            res[idx] = startTime[idx]\n",
    "\n",
    "        idx+=1\n",
    "    return res\n",
    "\n",
    "data['completeTime'] = calculator_nb(data['case'].values, data['startTime'].values)\n",
    "data.at[317373, 'completeTime'] = data.at[317373, 'startTime']\n",
    "\n",
    "data['startTime'] =  pd.to_datetime(data['startTime'])\n",
    "data['completeTime'] =  pd.to_datetime(data['completeTime'])\n",
    "data['duration'] = data['completeTime'] - data['startTime']\n",
    "#to turn duration into seconds:\n",
    "duration = data['duration']\n",
    "duration = duration / np.timedelta64(1, 's')\n",
    "data['duration'] = duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyyc-XNsAYCr"
   },
   "outputs": [],
   "source": [
    "#Next event\n",
    "@jit(parallel = True)\n",
    "def calculator_nb(case, event):\n",
    "    res = np.empty(len(case), dtype=object)\n",
    "    idx = 0\n",
    "    for _ in case:\n",
    "        if (idx+1 >= len(case)):\n",
    "            break\n",
    "       \n",
    "        if (case[idx + 1] == case[idx]):\n",
    "            res[idx] = event[idx + 1]\n",
    "\n",
    "        idx+=1\n",
    "    return res\n",
    "\n",
    "data['next_event'] = calculator_nb(data['case'].values, data['event'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxseoX0JAYCr"
   },
   "outputs": [],
   "source": [
    "#Previous event\n",
    "@jit(parallel = True)\n",
    "def calculator_nb(case, event):\n",
    "    res = np.empty(len(case), dtype=object)\n",
    "    idx = 0\n",
    "    for _ in case:\n",
    "        if (idx+1 >= len(case)):\n",
    "            break\n",
    "       \n",
    "        if (case[idx + 1] == case[idx]):\n",
    "            res[idx + 1] = event[idx]\n",
    "\n",
    "        idx+=1\n",
    "    return res\n",
    "\n",
    "data['prev_event'] = calculator_nb(data['case'].values, data['event'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lseT5ybaAYCs"
   },
   "outputs": [],
   "source": [
    "#Removing null values\n",
    "data['next_event'] = data['next_event'].fillna(value='None')\n",
    "data['prev_event'] = data['prev_event'].fillna(value='None')\n",
    "data['note'] = data['note'].fillna(value='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EK0yT10d4erc"
   },
   "outputs": [],
   "source": [
    "#unix time\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "data['startTime'] = pd.to_datetime(data['startTime'], dayfirst=True)\n",
    "unixTransform = lambda x: time.mktime(x.timetuple())\n",
    "data[\"UNIX_starttime\"] = data[\"startTime\"].apply(unixTransform).astype(int)\n",
    "\n",
    "data['completeTime'] = pd.to_datetime(data['completeTime'], dayfirst=True)\n",
    "unixTransform = lambda x: time.mktime(x.timetuple())\n",
    "data[\"UNIX_completeTime\"] = data[\"completeTime\"].apply(unixTransform).astype(int)\n",
    "\n",
    "#data['REG_DATE'] = pd.to_datetime(data['REG_DATE'], dayfirst=True)\n",
    "#unixTransform = lambda x: time.mktime(x.timetuple())\n",
    "#data[\"UNIX_REG_DATE\"] = data[\"REG_DATE\"].apply(unixTransform).astype(int)\n",
    "\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YGUvDZrAYCt"
   },
   "outputs": [],
   "source": [
    "#Day of the week\n",
    "data['weekday'] = data['startTime'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78FN-5HNNi4f"
   },
   "outputs": [],
   "source": [
    "#encoding of categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6l7QS1P6ndTr"
   },
   "outputs": [],
   "source": [
    "#ensure we have acces to orignal indexing to keep track of the order of events in a process\n",
    "data['original index'] = data.index\n",
    "\n",
    "#sorting on time\n",
    "data.sort_values(by = \"UNIX_starttime\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5nloqg7KspI"
   },
   "outputs": [],
   "source": [
    "#separation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDODvtbAJZad"
   },
   "outputs": [],
   "source": [
    "#removing overlap - if case is in both datasets, remove\n",
    "\n",
    "train_cases = train['case'].unique().tolist()\n",
    "test_cases = test['case'].unique().tolist()\n",
    "\n",
    "intersect_list = list(set(train_cases).intersection(test_cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9a3TLae6Vgc"
   },
   "outputs": [],
   "source": [
    "#only removes first value in intersect list (needs modification for multiple overlaping values)\n",
    "\n",
    "#train = train[train['case'] != intersect_list[0]]\n",
    "#test = test[test['case'] != intersect_list[0]]\n",
    "\n",
    "#works for more values\n",
    "org_train = train.copy()\n",
    "org_test = test.copy()\n",
    "train=train.apply(label_encoder.fit_transform)\n",
    "test=test.apply(label_encoder.fit_transform)\n",
    "\n",
    "train = train[train['case'].isin(intersect_list) == False]\n",
    "X_train_time = train.drop(columns='duration')\n",
    "Y_train_time = train[\"duration\"]\n",
    "X_train_event = train.drop(columns=[\"next_event\"])\n",
    "Y_train_event = train[\"next_event\"]\n",
    "\n",
    "test = test[test['case'].isin(intersect_list) == False]\n",
    "X_test_time = test.drop(columns='duration')\n",
    "Y_test_time = test[\"duration\"]\n",
    "X_test_event = test.drop(columns=['next_event'])\n",
    "Y_test_event = test[\"next_event\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCxZL9hWDs33"
   },
   "outputs": [],
   "source": [
    "#separation visualisation\n",
    "\n",
    "g = sns.scatterplot(x=\"UNIX_starttime\", y=\"case\", hue=\"enc_event\", data=data, palette='colorblind', legend=False)\n",
    "\n",
    "#add lines for separation - horizontal and vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJeqDuL6AYCv",
    "outputId": "60ac9040-5ca1-4e0c-c135-a0b9d38c39d8"
   },
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9BgcNRxAYCw"
   },
   "source": [
    "# Feature prediction for time and event based on KBest(z-scores)\n",
    "note: don't run takes a significant time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPCfrgnjAYCw"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "def calc_feature_selection():\n",
    "    select = SelectKBest(k=10)  # takes best 10 arguments\n",
    "    z = select.fit_transform(X_train_time, Y_train_time)\n",
    "    filter = select.get_support()\n",
    "    print(np.extract(filter, X_train_time.columns))\n",
    "    #['event' 'year' 'penalty_AVBP' 'penalty_AVGP' 'eventid' 'activity' 'docid''subprocess' 'success' 'next_event']\n",
    "\n",
    "    select = SelectKBest(k=10)  # takes best 10 arguments\n",
    "    z = select.fit_transform(X_train_event, Y_train_event)\n",
    "    filter = select.get_support()\n",
    "    print(np.extract(filter, X_train_event.columns))\n",
    "    #['event' 'selected_random' 'note' 'eventid' 'activity' 'subprocess''doctype' 'org:resource' 'duration' 'prev_event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_feature_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnszwsuyCw71"
   },
   "source": [
    "# Naive Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIsCchagAYCx"
   },
   "outputs": [],
   "source": [
    "# Naive event (needs restructuring)\n",
    "data_baseline= test.copy()\n",
    "\n",
    "@jit(parallel = True)\n",
    "def calculator_pos(case):\n",
    "    res = np.empty(len(case), dtype=object)\n",
    "    idx = 0\n",
    "    count=1\n",
    "    for _ in case:\n",
    "        if (idx+1 >= len(case)):\n",
    "            break\n",
    "       \n",
    "        if (case[idx] == case[idx-1]):\n",
    "            count+=1\n",
    "            res[idx] = count\n",
    "            \n",
    "        else:\n",
    "            count=1\n",
    "            res[idx]=count\n",
    "\n",
    "        idx+=1\n",
    "    res[-1]=count+1\n",
    "    return res\n",
    "\n",
    "data_baseline[\"pos\"] = calculator_pos(data_baseline['case'].values)\n",
    "\n",
    "event_to_num = {}\n",
    "list_of_events = train[\"event\"].unique()\n",
    "i=0\n",
    "for event in list_of_events:\n",
    "    event_to_num[str(event)] = i\n",
    "    i += 1\n",
    "event_to_num['None'] = i\n",
    "\n",
    "pop=data_baseline.sort_values(by='pos')\n",
    "pop['eventnum']=pop['enc_event']\n",
    "pop2=pop.set_index('pos')\n",
    "pop3=pop[['pos','eventnum']]\n",
    "pop4=pop3.groupby(['pos', 'eventnum']).apply(pd.DataFrame.mode).reset_index(drop=True)\n",
    "pop5=pop4.drop_duplicates(subset='pos')\n",
    "ptenum= dict(zip(pop5.pos, pop5.eventnum))\n",
    "num_to_event = {value:key for key, value in event_to_num.items()}\n",
    "data_baseline['predicted_event_num'] = (data_baseline['pos']+1).map(ptenum)\n",
    "data_baseline['predicted_event'] = (data_baseline['predicted_event_num']).map(num_to_event)\n",
    "data_baseline_final=data_baseline.drop(['predicted_event_num'],axis=1)\n",
    "\n",
    "next_task=[]\n",
    "predicted_event=[]\n",
    "for event in data_baseline_final['next_event']:\n",
    "    next_task.append(str(event))\n",
    "    \n",
    "\n",
    "for case in data_baseline_final['predicted_event']:\n",
    "    predicted_event.append(str(case))\n",
    "\n",
    "accuracy_score(next_task,predicted_event)\n",
    "\n",
    "\n",
    "test[\"naive_event\"] = predicted_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXmI8N5fAYCx"
   },
   "outputs": [],
   "source": [
    "# Naive time\n",
    "#Sums up count for each event and the time each event takes\n",
    "events_count = train.groupby(\"event\")['duration'].agg('count')\n",
    "event_duration_sum = train.groupby(\"event\")['duration'].agg('sum')\n",
    "\n",
    "#Computes average duration per event (basically our trained data that can be mapped onto test data)\n",
    "duration_per_event = event_duration_sum / events_count \n",
    "\n",
    "test[\"naive_time\"] = test['event'].map(duration_per_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzkbO8ioDJXG"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXp-EEClAYCy"
   },
   "outputs": [],
   "source": [
    "def calc_random_forest():\n",
    "    # Create the random grid\n",
    "\n",
    "    RF = RandomForestClassifier(n_estimators=300,\n",
    "                                min_samples_split=10,\n",
    "                                min_samples_leaf=2,\n",
    "                                max_features='sqrt',\n",
    "                                max_depth=50,\n",
    "                                bootstrap=True)\n",
    "\n",
    "    dataset_col = [\n",
    "        'event', 'selected_random', 'note', 'eventid', 'activity',\n",
    "        'subprocess', 'org:resource', 'duration', 'prev_event', 'enc_event'\n",
    "    ]\n",
    "\n",
    "    RF_fit = RF.fit(X_train_event.filter(items=dataset_col), Y_train_event)\n",
    "    RF_pred = RF_fit.predict(X_test_event.filter(items=dataset_col))\n",
    "    org_test[\"event_RF\"] = RF_pred\n",
    "    print(\"Accuracy for Random Forest: \",\n",
    "          accuracy_score(Y_test_event, RF_pred)) #91% accuracy on testing\n",
    "\n",
    "calc_random_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ95WWv1FFq2"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEjA1xPPAYCz"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxFBpvJrAYC0",
    "outputId": "afef450d-4463-4567-a3a0-8f40a49df81a"
   },
   "outputs": [],
   "source": [
    "def calc_LSTM():\n",
    "    listVal = train\n",
    "    columnNames = [\n",
    "        'event', 'penalty_AVBP', 'penalty_AVGP', 'eventid', 'activity',\n",
    "        'docid', 'subprocess', 'success', 'next_event', 'enc_event'\n",
    "    ]\n",
    "    listValSelected = listVal[columnNames]\n",
    "    listValSelected_prediction = test[columnNames]\n",
    "    listValSelected_prediction = listValSelected_prediction.values\n",
    "    listValDuration_prediction = org_test['duration']\n",
    "    listValDuration_prediction = listValDuration_prediction.values\n",
    "    listValDuration = org_train['duration'].values\n",
    "\n",
    "    listValSelected = listValSelected.values\n",
    "    n_steps = len(listValSelected[0])\n",
    "    # split into samples\n",
    "    n_features = 1\n",
    "    X = listValSelected.reshape(\n",
    "        (listValSelected.shape[0], listValSelected.shape[1], n_features))\n",
    "    y = listValDuration\n",
    "\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        LSTM(\n",
    "            100,\n",
    "            input_shape=(n_steps, n_features),\n",
    "            #  stateful=True,\n",
    "            return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(LSTM(units=100))\n",
    "    # model.add(Dropout(0.2))\n",
    "    # model.add(Dense(units=1))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    model.fit(X,\n",
    "              y,\n",
    "              batch_size=100,\n",
    "              epochs=10,\n",
    "              verbose=1,\n",
    "              workers=-1,\n",
    "              use_multiprocessing=True)\n",
    "\n",
    "    # demonstrate prediction\n",
    "    x_input = listValSelected_prediction\n",
    "\n",
    "    x_input = x_input.reshape((x_input.shape[0], n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=1, use_multiprocessing=True)\n",
    "    org_test[\"time_LSTM\"] = yhat.flatten()[:len(listValDuration_prediction)]\n",
    "    print(\n",
    "        mean_absolute_error(listValDuration_prediction,\n",
    "                            yhat.flatten()[:len(listValDuration_prediction)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8VnfD11COk3"
   },
   "source": [
    "<h1>Neural network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['event' 'selected_random' 'note' 'eventid' 'activity' 'subprocess'\n",
    "# 'doctype' 'org:resource' 'duration' 'prev_event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#event note duration prev event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mAQfBJXA5e4"
   },
   "outputs": [],
   "source": [
    "def normalize(df_name, col_name):\n",
    "    col_as_array = df_name[col_name].to_numpy()\n",
    "    col_as_array = np.where(col_as_array == 0, 0.01, col_as_array)\n",
    "    col_as_array_norm = np.log10(col_as_array)\n",
    "    mean = col_as_array_norm.mean()\n",
    "    stdev = col_as_array_norm.std()\n",
    "    epsilon = 0.01\n",
    "    return (col_as_array_norm - mean) / (stdev + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjaBr85RBK9u"
   },
   "outputs": [],
   "source": [
    "def prepfeatures(df_name):\n",
    "    event = df_name['event'].to_numpy()\n",
    "    event = event.reshape(-1,1)\n",
    "    event = ordinal_encoder.fit_transform(event)\n",
    "    \n",
    "    #selected_random = df_name['selected_random'].to_numpy()\n",
    "    #selected_random = selected_random.reshape(-1,1)\n",
    "    #selected_random = ordinal_encoder.fit_transform(selected_random)\n",
    "    \n",
    "    #note = df_name['note'].to_numpy()\n",
    "    #note = note.reshape(-1,1)\n",
    "    #note = ordinal_encoder.fit_transform(note)\n",
    "    \n",
    "    #eventid = org_train['eventid'].to_numpy()\n",
    "    #eventid = eventid + abs(org_train['eventid'].min())\n",
    "    \n",
    "    #subprocess = df_name['subprocess'].to_numpy()\n",
    "    #subprocess = subprocess.reshape(-1,1)\n",
    "    #subprocess = ordinal_encoder.fit_transform(subprocess)\n",
    "    \n",
    "    #doctype = df_name['doctype'].to_numpy()\n",
    "    #doctype = doctype.reshape(-1,1)\n",
    "    #doctype = ordinal_encoder.fit_transform(doctype)\n",
    "    \n",
    "    duration = normalize(df_name,'duration')\n",
    "    \n",
    "    startTime = normalize(df_name,'UNIX_starttime')\n",
    "    \n",
    "    weekday = df_name['weekday'].to_numpy()\n",
    "    \n",
    "    prev_event = df_name['prev_event'].to_numpy()\n",
    "    prev_event = prev_event.reshape(-1,1)\n",
    "    prev_event = ordinal_encoder.fit_transform(prev_event)\n",
    "    \n",
    "    features = []\n",
    "    for i in range(len(event)):\n",
    "        current = event[i]\n",
    "        #current = np.append(current,selected_random[i])\n",
    "        #current = np.append(current,note[i])\n",
    "        #current = np.append(current,eventid[i])\n",
    "        #current = np.append(current,subprocess[i])\n",
    "        #current = np.append(current,doctype[i])\n",
    "        current = np.append(current,duration[i])\n",
    "        current = np.append(current,startTime[i])\n",
    "        current = np.append(current,weekday[i])\n",
    "        current = np.append(current,prev_event[i])\n",
    "        features.append(current)\n",
    "        \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtJzg5ZPBNhL"
   },
   "outputs": [],
   "source": [
    "def preplabels(df_name):\n",
    "    labels = df_name['next_event'].to_numpy()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    \n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "On8mJnByBPbR"
   },
   "outputs": [],
   "source": [
    "features = prepfeatures(org_train)\n",
    "labels = preplabels(org_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szrad1aBBRNR"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(5,)),\n",
    "    keras.layers.Dense(10, activation='softplus'),\n",
    "    keras.layers.Dense(15, activation='softplus'),\n",
    "    keras.layers.Dense(20, activation='softplus'),\n",
    "    keras.layers.Dropout(1/20),\n",
    "    keras.layers.Dense(25, activation='softplus'),\n",
    "    keras.layers.Dense(30, activation='softplus'),\n",
    "    keras.layers.Dropout(1/30),\n",
    "    keras.layers.Dense(35, activation='softplus'),\n",
    "    keras.layers.Dense(42, activation='softplus')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCwPArxCBTj1"
   },
   "outputs": [],
   "source": [
    "model.fit(features,labels,epochs=3,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgYm7vyHBWYL"
   },
   "outputs": [],
   "source": [
    "features_test = prepfeatures(org_test)\n",
    "labels_test = preplabels(org_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = model.evaluate(features_test,labels_test)\n",
    "print(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G02fgUc3BjNV"
   },
   "outputs": [],
   "source": [
    "test['neuralnet_event'] = model.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMwKREQNFNMU"
   },
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujVonZkbFUFB"
   },
   "outputs": [],
   "source": [
    "def prepfeatures_regression(df_name):\n",
    "    event = df_name['event'].to_numpy()\n",
    "    event = event.reshape(-1,1)\n",
    "    event = ordinal_encoder.fit_transform(event)\n",
    "    \n",
    "    next_event = df_name['next_event'].to_numpy()\n",
    "    next_event = next_event.reshape(-1,1)\n",
    "    next_event = ordinal_encoder.fit_transform(next_event)\n",
    "    \n",
    "    year = df_name['year'].to_numpy()\n",
    "    year = year.reshape(-1,1)\n",
    "    year = ordinal_encoder.fit_transform(year)\n",
    "    \n",
    "    penalty_AVBP = df_name['penalty_AVBP'].to_numpy()\n",
    "    penalty_AVBP = penalty_AVBP.reshape(-1,1)\n",
    "    penalty_AVBP = ordinal_encoder.fit_transform(penalty_AVBP)\n",
    "    \n",
    "    penalty_AVGP = df_name['penalty_AVGP'].to_numpy()\n",
    "    penalty_AVGP = penalty_AVGP.reshape(-1,1)\n",
    "    penalty_AVGP = ordinal_encoder.fit_transform(penalty_AVGP)\n",
    "    \n",
    "    success = df_name['success'].to_numpy()\n",
    "    success = success.reshape(-1,1)\n",
    "    success = ordinal_encoder.fit_transform(success)\n",
    "    \n",
    "    eventid = df_name['eventid'].to_numpy()\n",
    "    eventid = success.reshape(-1,1)\n",
    "    \n",
    "    docid = df_name['docid'].to_numpy()\n",
    "    docid = success.reshape(-1,1)\n",
    "    \n",
    "    subprocess = df_name['subprocess'].to_numpy()\n",
    "    subprocess = subprocess.reshape(-1,1)\n",
    "    subprocess = ordinal_encoder.fit_transform(subprocess)\n",
    "    \n",
    "    weekday = df_name['weekday'].to_numpy()\n",
    "    weekday = weekday.reshape(-1,1)\n",
    "    \n",
    "    X = []\n",
    "    for i in range(len(event)):\n",
    "        current = event[i]\n",
    "        current = np.append(current, year[i])\n",
    "        current = np.append(current, penalty_AVBP[i])\n",
    "        current = np.append(current, penalty_AVGP[i])\n",
    "        current = np.append(current, success[i])\n",
    "        current = np.append(current, eventid[i])\n",
    "        current = np.append(current, docid[i])\n",
    "        current = np.append(current, next_event[i])\n",
    "        current = np.append(current, subprocess[i])\n",
    "        X.append(current)\n",
    "        \n",
    "    return np.array(X, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_VXHw5DFaAI"
   },
   "outputs": [],
   "source": [
    "def preplabels_regression(df_name):\n",
    "    duration = df_name['duration'].to_numpy()\n",
    "    return np.array(duration, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = prepfeatures_regression(org_train)\n",
    "y = preplabels_regression(org_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0Qh176sFcMT"
   },
   "outputs": [],
   "source": [
    "huber = HuberRegressor().fit(X, y)\n",
    "\n",
    "X_test = prepfeatures_regression(org_test)\n",
    "\n",
    "org_test['regression_duration'] = huber.predict(X_test)\n",
    "org_test['error'] = np.absolute(org_test['duration'] - org_test['regression_duration'])\n",
    "org_test['error'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_prediction = prepfeatures_regression(data)\n",
    "data['regression_time_prediction'] = huber.predict(X_for_prediction)\n",
    "data['regression_time_prediction'] = data['regression_time_prediction'] + data['UNIX_starttime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['regression_time_predicition'] = data['regression_time_prediction'].apply(datetime.fromtimestamp)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sprint3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
