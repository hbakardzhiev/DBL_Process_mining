{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
      ],
      "metadata": {
        "id": "RiqykWRP2orS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmv9VAyTs_sJ"
      },
      "outputs": [],
      "source": [
        "def time_metrics(y_test, y_pred, model=\"...\"):\n",
        "  \n",
        "    print(\"\\n\")\n",
        "    print(f\"Error metrics for the {model} time model\")\n",
        "    print(\"\\n\")\n",
        "    print('Mean Absolute Error:', mean_absolute_error(y_test, y_pred))\n",
        "    print('Mean Squared Error:', mean_squared_error(y_test, y_pred))\n",
        "    print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "    print('$R_2$ score:', r2_score(y_test, y_pred)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXR9a-ifs_sL"
      },
      "outputs": [],
      "source": [
        "def event_metrics(y_test, y_pred, avg=\"weighted\", model=\"...\"):\n",
        " \n",
        "    prec_score = precision_score(y_test, y_pred, average=avg, zero_division=0)\n",
        "    rec_score = recall_score(y_test, y_pred, average=avg, zero_division=0)\n",
        "    F1_score = f1_score(y_test, y_pred, average=avg, zero_division=0)\n",
        "    acc_score = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(f\"Error metrics for the {model} event model\")\n",
        "    print(\"\\n\")\n",
        "    print(f'The accuracy of the model is {acc_score}.')\n",
        "    print(f'The precision of the model is {prec_score}, using {avg} average.')\n",
        "    print(f'The recall of the model is {rec_score}, using {avg} average.')\n",
        "    print(f'The f1-score of the model is {F1_score}, using {avg} average.')\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return acc_score, prec_score, rec_score, F1_score"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "47c473ffaad557a40d6f692199c8550b37e2966a5f36ac429a864ed95aaad2b0"
    },
    "kernelspec": {
      "display_name": "Python 3.9.11 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "error 'model'.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}